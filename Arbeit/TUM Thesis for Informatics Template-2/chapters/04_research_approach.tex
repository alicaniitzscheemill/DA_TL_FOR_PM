\chapter{Research Approach}\label{chapter:research_approach}

When industrial machines run over long time horizons, operational conditions and therefore fault characteristics might change. Reasons for that can be abrasion, changed machine settings or minor installation differences when submodules of the machine were replaced. Hand-crafted features, as they are mostly used in the traditional approaches, expect a lot of experience and human labor to be developed appropriately. Due to the lack of flexibility and robustness, these hand-crafted features will most certainly fail with changing fault characteristics. Deep-learning based PHM systems might have better chances to find more general and expressive features. Anyhow, due to the complex correlation and dependency between different machine submodules, faults can have numerous causes and characteristics. Therefore, it is unlikely that the data used for training the model includes all operational conditions and fault scenarios. It can even happen that some fault classes are unknown during training. When training neural networks on data, which is recorded during a limited amount of time, this might lead to unsatisfactory diagnosis performance while testing \cite{AZAMFAR2020103932}. Robust PHM systems, which can handle variations in fault characteristics or even unseen fault classes, would bring industrial PHM systems to a next level \cite{Michau2017}. In order to address those issues, domain adaption approaches seem promising in this context. This thesis investigates the applicability and usability of PHM systems using domain adaption for identifying degradation levels of BSDs. The advantages over traditional data-driven methods is evaluated. Most domain adaption approaches, just as the one presented by Azamfar et al \cite{AZAMFAR2020103932} and Pandhare et al \cite{Pandhare2021}, reduce the domain discrepancy in task-specific layers but use a shared feature extractor backbone across all domains. Li et al \cite{li2020} assume that, if the domain discrepancy is tremendously large, these methods can only reduce the domain discrepancy, but not fundamentally eliminate it. For this reason, this thesis investigates how applying the MMD loss in the feature extractor can help to reduce the discrepancy more efficiently challenging tasks. Since the source and target domains are correlated to some extend, the network itself can extract domain-independent features. The powerful feature extractor learned from the source domain can also increase the model performance on the target domain. At the same time, features which are too sensitive to the source domain can reduce the model performance on the target domain \cite{li2020}. To counteract that phenomena, domain adaption approaches can help to transfer knowledge learned on the source to the target domain. Anyhow, one has to pay attention to not transfer noise or irrelevant information, since this destroys the structure of the source and target domain data and makes the classification task even more difficult \cite{li2020}. It is important to balance the effect of the MMD- and CE-loss very sensible. In this thesis the effects of different GAMMA choices are investigated. Pandhare et al \cite{Pandhare2021} apply PD-alignment, which specifically reduces the L2-distance between samples belonging to the same class but different domains. This increases the domain overlap in the latent feature space. With an increasing domain similarity, the extraction of domain invariant features becomes easier. Unfortunately, the labels of the source and target domain samples need to be known to apply the PD-alignment. In theory the positive effects of PD-alignment are obvious. This thesis analyzes how target labels can improve the MMD-based domain adaption capabilities. This thesis does not apply a PD-alignment in addition to a MMD-loss but rather more develops a novel MMD-loss which considers the classes of source and target domain samples. Like in the work of Pandhare et al \cite{Pandhare2021}, the target labels are not used in the CE-loss. Since it is difficult to compare PHM systems which have access to different data, the PHM approaches evaluated on the real-world dataset should be restricted to source domain labels. Li et al \cite{Li2018} present a PHM algorithm for rolling bearings, which optimizes the inter- and intra-class distance in the latent feature space and reduces the domain discrepancy with an MMD-loss. For all samples of the same class the expectation and variance is measured in the feature maps of interest. The expectation and variance are used to optimize the intra- and inter-class distances for the source domain. This approach by Li et al could be an option for a distance-based optimization to increase the compactness and separability in the latent feature spaces without relying on target domain data. Due to the restriction to the source domain, it's effectiveness for a domain adaption task is questionable. In total three main research questions were targeted throughout the thesis:
\begin{itemize}
    \item [1.] Influence of latent feature space choice on the domain adaption performance
    \item [2.] Influence of GAMMA choice on the domain adaption performance
    \item [3.] Domain adaption performance when using a labeled MMD-loss
\end{itemize}

There are several domain adaption approaches for PHM of rolling bearing based on MMD-losses \cite{Guo2019} \cite{Singh2019} \cite{Li2018} \cite{AN201942} \cite{Kang2020}. Generally, BSDs and rolling bearing are related components. The BSD shaft can be seen as the inner ring and the BSD nut as the outer ring of a rolling bearing. In both cases balls create a movable bearing between those two parts along a fixed axis. Other than bearings, BSDs also translate this rotatory motion in a linear motion between BSD shaft and nut. In general the degradation of these two parts is related in some sense. Nevertheless, bearing PHM applications cannot be relied to work well for BSDs. Still the research in this domain still offers a bunch of interesting applications and details for BSD PHM. 

Besides that there are also quite interesting applications which use multi-adversarial networks \cite{Zhang2019} or deep belief networks (DBN) \cite{ZHAO2019213} for PHM of different industrial components. When training general adversarial neural networks (GANs), two networks, which work against each other, need to be optimized simultaneously. Deep belief networks (DBN) contain several stacked restricted Boltzmann machines (RBMs). The training of DBNs is separated in two phases. First, all RBMs are optimized individually. Afterwards, the DBN is fine-tuned to solve the classification task by applying backpropagation on all RBM layers simultaneously. Especially when applying such networks on noisy and disturbed real-world vibration signals, this might lead to instabilities. For this reason and due to the limited time of this thesis, MMD-losses are in the focus of this thesis.

