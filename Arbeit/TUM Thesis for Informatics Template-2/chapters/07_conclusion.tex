\chapter{Conclusion}\label{chapter:conclusion}


In this thesis, a deep learning-based PHM system for monitoring BSDs was successfully extended with a domain adaptation module. 


In this thesis, a deep learning-based PHM system for monitoring BSDs was successfully extended with a domain adaptation module. A novel way of reducing the domain discrepancy in the network's feature extractor was proposed. Compared to the traditional approaches, which solely reduce the domain discrepancy in the task specific layers, the application of the MMD-loss in the layers of the CNN reduced the domain discrepancy more efficiently. This greater efficiency was mainly reflected in the overall model performance and the increased stability during the training. A novel labeled MMD-loss, which considered the source and target labels, reduced the domain discrepancy between samples of the same class and increased it between those of different classes. This guaranteed an improved compactness and separability of all classes while reducing the domain discrepancy. The labeled MMD-loss revealed the main deficit of the unlabeled MMD-loss, which was the reduction of the domain discrepancy between the samples of all classes. When the unlabeled MMD-loss becomes too dominant during training, the separability was reduced and the structure of the data destroyed. In this case the representations of the samples in the latent feature space collapsed to a point- or needle-like subspace, which made the classification more challenging. Therefore, the GAMMA choice was highly relevant for the PHM performance and had to be picked carefully and individually for each signal. An imperfect GAMMA choice led to a trivial optimization solution, where the feature representations of all samples collapsed to a small subset.


In this thesis, a deep learning-based PHM system for monitoring BSDs was successfully extended with a domain adaptation module. A novel way of reducing the domain discrepancy in the network's feature extractor was proposed. Compared to the traditional approaches, which solely reduce the domain discrepancy in the task specific layers, the application of the MMD-loss in the layers of the CNN reduced the domain discrepancy more efficiently. This greater efficiency was mainly reflected in the overall model performance and the increased stability during the training. A novel labeled MMD-loss, which considered the source and target labels, reduced the domain discrepancy between samples of the same class and increased it between those of different classes. This guaranteed an improved compactness and separability of all classes while reducing the domain discrepancy. The labeled MMD-loss revealed the main deficit of the unlabeled MMD-loss, which was the reduction of the domain discrepancy between the samples of all classes. When the unlabeled MMD-loss becomes too dominant during training, the separability was reduced and the structure of the data destroyed. In this case the representations of the samples in the latent feature space collapsed to a point- or needle-like subspace, which made the classification more challenging. Therefore, the GAMMA choice was highly relevant for the PHM performance and had to be picked carefully and individually for each signal. An imperfect GAMMA choice led to a trivial optimization solution, where the feature representations of all samples collapsed to a small subset.
